{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a char-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info//shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt',shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text into a vector of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "1115394\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count \n",
    "\n",
    "print(max_id)\n",
    "print(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 #subtract 1 to get 0->N instead of 1->N+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90//100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100 #window the data to generate \"batches\"\n",
    "window_length  = n_steps + 1\n",
    "dataset = dataset.window(window_length,shift = 1,drop_remainder = True)\n",
    "\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length)) #flatten the dataset into window_length sized groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1],windows[:,1:])) #shuffle the data, turn each dataset into a train/test of 100 samples long each (i.e. 0-100 train, 1-101 test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch,y_batch: (tf.one_hot(X_batch, depth = max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31368/31368 [==============================] - 677s 21ms/step - loss: 1.6215\n",
      "Epoch 2/10\n",
      "31368/31368 [==============================] - 682s 22ms/step - loss: 1.5393\n",
      "Epoch 3/10\n",
      "31368/31368 [==============================] - 615s 20ms/step - loss: 1.5190\n",
      "Epoch 4/10\n",
      "31368/31368 [==============================] - 668s 21ms/step - loss: 1.5069\n",
      "Epoch 5/10\n",
      "31368/31368 [==============================] - 702s 22ms/step - loss: 1.4998\n",
      "Epoch 6/10\n",
      "31368/31368 [==============================] - 668s 21ms/step - loss: 1.4946\n",
      "Epoch 7/10\n",
      "31368/31368 [==============================] - 645s 21ms/step - loss: 1.4910\n",
      "Epoch 8/10\n",
      "31368/31368 [==============================] - 660s 21ms/step - loss: 1.4880\n",
      "Epoch 9/10\n",
      "31368/31368 [==============================] - 713s 23ms/step - loss: 1.4853\n",
      "Epoch 10/10\n",
      "31368/31368 [==============================] - 705s 22ms/step - loss: 1.4828\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128,return_sequences = True,dropout = 0.2,input_shape = [None,max_id]),\n",
    "    keras.layers.GRU(128,return_sequences = True,dropout = 0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation = 'softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',optimizer='adam')\n",
    "history = model.fit(dataset,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    token = np.array(tokenizer.texts_to_sequences(text)) - 1\n",
    "    return tf.one_hot(token,max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = preprocess(['How are yo'])\n",
    "y_pred = np.argmax(model(X_test),axis = -1)\n",
    "tokenizer.sequences_to_texts(y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('ch16_charRnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text,temperature = 1):\n",
    "    Xnew = preprocess([text])\n",
    "    \n",
    "    yproba = model(Xnew)\n",
    "    \n",
    "    rescaled_logits = tf.math.log(yproba[0,:-1,:]) / temperature \n",
    "    \n",
    "    char_id = tf.random.categorical(tf.reshape(rescaled_logits,[1,len(rescaled_logits)]), num_samples = 1, seed = 42) + 1\n",
    "    #char_id = tf.random.categorical(rescaled_logits, num_samples = 1, seed = 42) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text,n_chars =100,temperature = 1):\n",
    "    for _ in range(n_chars):\n",
    "        text +=  next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 1, 39), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[9.74714607e-02 2.67819059e-03 1.19457170e-01 2.33859406e-03\n",
      "   2.02142796e-03 3.47330272e-02 3.63231753e-03 9.82685089e-02\n",
      "   8.52513909e-02 1.88041374e-01 5.36741223e-03 7.62311518e-02\n",
      "   2.75591966e-02 2.38994546e-02 3.75987515e-02 2.03163400e-02\n",
      "   1.04723815e-02 1.38938036e-02 1.99915841e-02 1.31668365e-02\n",
      "   7.94183742e-03 1.19786318e-02 1.47622144e-02 2.15537865e-02\n",
      "   1.09882755e-02 3.26979458e-02 2.62448844e-03 4.43814788e-03\n",
      "   1.99976284e-03 6.08921330e-03 1.57334853e-03 1.29372784e-04\n",
      "   5.65733098e-05 4.54121728e-05 1.48899868e-04 5.81624627e-04\n",
      "   5.27938333e-08 8.27294322e-09 9.26592847e-09]]], shape=(1, 1, 39), dtype=float32)\n",
      "tf.Tensor([], shape=(0, 39), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "num_classes should be positive, got 0 [Op:Multinomial]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gerardo\\Documents\\Projects\\PythonProjects\\ML_Book\\ch16.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000020?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(complete_text(\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m,temperature \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\Gerardo\\Documents\\Projects\\PythonProjects\\ML_Book\\ch16.ipynb Cell 20'\u001b[0m in \u001b[0;36mcomplete_text\u001b[1;34m(text, n_chars, temperature)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomplete_text\u001b[39m(text,n_chars \u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,temperature \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000019?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_chars):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000019?line=2'>3</a>\u001b[0m         text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m  next_char(text, temperature)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000019?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n",
      "\u001b[1;32mc:\\Users\\Gerardo\\Documents\\Projects\\PythonProjects\\ML_Book\\ch16.ipynb Cell 19'\u001b[0m in \u001b[0;36mnext_char\u001b[1;34m(text, temperature)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000018?line=5'>6</a>\u001b[0m rescaled_logits \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mlog(yproba[\u001b[39m0\u001b[39m,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]) \u001b[39m/\u001b[39m temperature \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000018?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(rescaled_logits)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000018?line=7'>8</a>\u001b[0m char_id \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mcategorical(tf\u001b[39m.\u001b[39;49mreshape(rescaled_logits,[\u001b[39m1\u001b[39;49m,\u001b[39mlen\u001b[39;49m(rescaled_logits)]), num_samples \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, seed \u001b[39m=\u001b[39;49m \u001b[39m42\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000018?line=8'>9</a>\u001b[0m \u001b[39m#char_id = tf.random.categorical(rescaled_logits, num_samples = 1, seed = 42) + 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000018?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39msequences_to_texts(char_id\u001b[39m.\u001b[39mnumpy())[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/framework/ops.py?line=7183'>7184</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/framework/ops.py?line=7184'>7185</a>\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Gerardo/.virtualenvs/ml/lib/site-packages/tensorflow/python/framework/ops.py?line=7185'>7186</a>\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: num_classes should be positive, got 0 [Op:Multinomial]"
     ]
    }
   ],
   "source": [
    "print(complete_text('a',temperature = 1))\n",
    "#print(complete_text('w',temperature = 1))\n",
    "#print(complete_text('w',temperature = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is to keep the hidden state across batches. Numerous complications regarding batching, so easiest solution is to create batches of size = 1.\n",
    "Also important to reset states after every epoch, since these are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift = n_steps, drop_remainder = True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:,1:]))\n",
    "dataset = dataset.map(lambda X_batch, y_batch : (tf.one_hot(X_batch,depth = max_id),y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.model.Sequential([\n",
    "    keras.layers.GRU(128,return_sequences = True, stateful = True,\n",
    "                     dropout = 0.2, batch_input_shape = [batch_size , None, max_id]),\n",
    "    keras.layers.GRU(128,return_sequences = True, stateful = True,\n",
    "                     dropout = 0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation = 'softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStateCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epoch,logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'adam')\n",
    "model.fit(dataset, epochs = 50, callbacks = [ResetStateCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Gerardo\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 80/80 [00:14<00:00,  5.67 MiB/s]rl]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:14<00:00, 14.11s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Gerardo\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews',as_supervised = True, with_info = True)\n",
    "train_size = info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch,y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\",b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z'\",b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    \n",
    "    return X_batch.to_tensor(default_value = b'<pad>'), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gerardo\\Documents\\Projects\\PythonProjects\\ML_Book\\ch16.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000033?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000033?line=1'>2</a>\u001b[0m vocabulary \u001b[39m=\u001b[39m Counter()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000033?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m X_batch, y_batch \u001b[39min\u001b[39;00m datasets[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mbatch(\u001b[39m32\u001b[39m)\u001b[39m.\u001b[39mmap(preprocess):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000033?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m X_batch:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000033?line=4'>5</a>\u001b[0m         vocabulary\u001b[39m.\u001b[39mupdate(\u001b[39mlist\u001b[39m(review\u001b[39m.\u001b[39mnumpy()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gerardo\\Documents\\Projects\\PythonProjects\\ML_Book\\ch16.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch16.ipynb#ch0000034?line=0'>1</a>\u001b[0m vocabulary\u001b[39m.\u001b[39mmost_common()[:\u001b[39m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocab = [word for word,count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocab)\n",
    "words_ids = tf.range(len(truncated_vocab),dtype = tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words,words_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "table.lookup(tf.constant([b'This movie was faaaaantastic'.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch,y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets['train'].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets,embed_size,input_shape = [None]),\n",
    "    keras.layers.GRU(128,return_sequences = True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1,activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n",
    "history = model.fit(train_set,epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ffb14accc1fce094d0277faef16b62359a1687f3276fab306fad88c16a000e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
