{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y = load_iris(return_X_y=True)\n",
    "#per_clf = Perceptron().fit(X[:,(2,3)],y)\n",
    "\n",
    "#per_clf .predict([[2,5]])x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full,y_train_full),(X_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255. , X_train_full[5000:] / 255.\n",
    "y_valid , y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.\n",
    "\n",
    "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "model.add(keras.layers.Dense(300,activation = 'relu'))\n",
    "model.add(keras.layers.Dense(100,activation = 'relu'))\n",
    "model.add(keras.layers.Dense(10,activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 7s 3ms/step - loss: 0.7075 - accuracy: 0.7685 - val_loss: 0.5308 - val_accuracy: 0.8164\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4857 - accuracy: 0.8309 - val_loss: 0.5155 - val_accuracy: 0.8126\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4417 - accuracy: 0.8452 - val_loss: 0.4089 - val_accuracy: 0.8618\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4147 - accuracy: 0.8555 - val_loss: 0.4094 - val_accuracy: 0.8602\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3938 - accuracy: 0.8610 - val_loss: 0.3996 - val_accuracy: 0.8646\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3768 - accuracy: 0.8680 - val_loss: 0.3664 - val_accuracy: 0.8724\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3650 - accuracy: 0.8715 - val_loss: 0.3719 - val_accuracy: 0.8694\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3531 - accuracy: 0.8747 - val_loss: 0.3501 - val_accuracy: 0.8762\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3431 - accuracy: 0.8781 - val_loss: 0.3444 - val_accuracy: 0.8786\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3333 - accuracy: 0.8810 - val_loss: 0.3497 - val_accuracy: 0.8804\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3238 - accuracy: 0.8845 - val_loss: 0.3465 - val_accuracy: 0.8762\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3165 - accuracy: 0.8870 - val_loss: 0.3378 - val_accuracy: 0.8786\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3094 - accuracy: 0.8902 - val_loss: 0.3254 - val_accuracy: 0.8848\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3037 - accuracy: 0.8905 - val_loss: 0.3293 - val_accuracy: 0.8870\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2960 - accuracy: 0.8949 - val_loss: 0.3311 - val_accuracy: 0.8824\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2899 - accuracy: 0.8959 - val_loss: 0.3483 - val_accuracy: 0.8754\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2843 - accuracy: 0.8974 - val_loss: 0.3385 - val_accuracy: 0.8770\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2788 - accuracy: 0.8990 - val_loss: 0.3194 - val_accuracy: 0.8884\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2734 - accuracy: 0.9019 - val_loss: 0.3435 - val_accuracy: 0.8772\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2672 - accuracy: 0.9040 - val_loss: 0.3288 - val_accuracy: 0.8810\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2628 - accuracy: 0.9061 - val_loss: 0.2982 - val_accuracy: 0.8946\n",
      "Epoch 22/30\n",
      " 264/1719 [===>..........................] - ETA: 4s - loss: 0.2519 - accuracy: 0.9086"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gerardo\\Documents\\Projects\\PythonProjects\\ML_Book\\ch10.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch10.ipynb#ch0000005?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,optimizer \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m,metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gerardo/Documents/Projects/PythonProjects/ML_Book/ch10.ipynb#ch0000005?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m  model\u001b[39m.\u001b[39;49mfit(X_train,y_train,epochs \u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m,validation_data \u001b[39m=\u001b[39;49m (X_valid,y_valid))\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\.virtualenvs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///~/.virtualenvs/ml/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',optimizer ='sgd',metrics = ['accuracy'])\n",
    "history =  model.fit(X_train,y_train,epochs = 30,validation_data = (X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.ylim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_prob = model.predict(X_new)\n",
    "print(y_prob.round(2))\n",
    "\n",
    "print(np.array(class_names)[np.argmax(y_prob,axis=1)])\n",
    "\n",
    "for s in range(3):\n",
    "    plt.subplot(1,3,s+1); plt.imshow(X_new[s],cmap = 'gray'); plt.xticks([]); plt.yticks([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_full,y_full = fetch_california_housing(return_X_y= True)\n",
    "X_train,X_tmp,y_train,y_tmp = train_test_split(X_full,y_full,test_size = .2)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_tmp,y_tmp,test_size = .2)\n",
    "\n",
    "my_scaler = StandardScaler()\n",
    "\n",
    "X_train = my_scaler.fit_transform(X_train)\n",
    "X_val = my_scaler.transform(X_val)\n",
    "X_test = my_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation='relu',input_shape = X_train[0].shape),\n",
    "    keras.layers.Dense(1) #only output of 1, without activation function, since it's a regression task\n",
    "])\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "history = model.fit(X_train,y_train,epochs=20,validation_data = (X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(X_test,y_test))\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "print(f'Predictions: {y_pred.flatten()}')\n",
    "print(f'Actual values: {y_test[:3].flatten()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, notice how we are able to \"pipe\" the input_ layer straight to our output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape = X_train[0].shape)\n",
    "hidden1 = keras.layers.Dense(30, activation = 'relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation = 'relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=[input_], outputs = [output])\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer=keras.optimizers.SGD(learning_rate = 1e-3))\n",
    "history = model.fit(X_train,y_train,epochs = 20, validation_data = (X_val,y_val),verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_ = keras.layers.Input(shape = [6],  name = 'Deep_input')\n",
    "input2_ = keras.layers.Input(shape = [5] , name = 'Wide_input')\n",
    "\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input1_)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "\n",
    "concat = keras.layers.Concatenate()([input2_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.Model(inputs = [input1_, input2_], outputs= [output])\n",
    "model.compile(loss='mean_squared_error',optimizer = keras.optimizers.SGD(learning_rate= 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: 6 features, B: 5 features\n",
    "X_train_A, X_train_B = X_train[:,:5], X_train[:,2:]\n",
    "X_val_A, X_val_B = X_val[:,:5], X_val[:,2:]\n",
    "X_test_A, X_test_B = X_test[:,:5], X_test[:,2:]\n",
    "X_new_A, X_new_B = X_test_A[:3],X_test_B[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the tuple for inputs passed in fit. And in the creation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit((X_train_B,X_train_A),y_train,epochs = 20, validation_data = ((X_val_B,X_val_A),y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = model.evaluate((X_test_B,X_test_A),y_test)\n",
    "y_preds = model.predict((X_new_B,X_new_A))\n",
    "\n",
    "print(mse_test)\n",
    "print(y_preds.flatten())\n",
    "print(y_test[:3].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5],name = 'wide')\n",
    "input_B = keras.layers.Input(shape = [6],name = 'deep')\n",
    "\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "\n",
    "concat = keras.layers.Concatenate()([input_A , hidden2])\n",
    "output = keras.layers.Dense(1, name = 'main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1,name = 'aux_output')(hidden2)\n",
    "\n",
    "model = keras.Model(inputs=[input_A,input_B],outputs = [output, aux_output])\n",
    "model.compile(loss = ['mse','mse'],loss_weights = [.9, .1],optimizer = 'sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we pass multiple loss (can be the same) for different outputs. Also, but adding \"loss_weights\", we ensure the auxiliary output is not as valuable as the main output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit((X_train_A,X_train_B),(y_train,y_train),epochs = 20, validation_data = ((X_val_A,X_val_B) , (y_val,y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A,X_test_B),(y_test,y_test))\n",
    "print(f'Total loss: {total_loss}')\n",
    "print(f'Main loss: {main_loss}')\n",
    "print(f'Aux loss: {aux_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main,y_pred_aux = model.predict([X_new_A,X_new_B])\n",
    "print(y_pred_main.flatten())\n",
    "print(y_pred_aux.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and prediction will also return 2 outputs (main and auxiliary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubClassing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideandDeepModel(keras.Model):\n",
    "    def __init__(self,units=30,activation='relu',**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units,activation)\n",
    "        self.hidden2 = keras.layers.Dense(units,activation)\n",
    "        self.main_output = keras.layers.Dense(1,name = 'main_output')\n",
    "        self.aux_output = keras.layers.Dense(1,name = 'aux_output')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        input_A, input_B = inputs\n",
    "        \n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        \n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        \n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        \n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subclassing allows the implementation of the model with any input layer. Also any sort of programming logic embedded in the \"call\" method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= WideandDeepModel()\n",
    "model.compile(loss = 'mean_squared_error',optimizer = 'sgd')\n",
    "history = model.fit((X_train_A,X_train_B),(y_train,y_train), epochs = 20, validation_data = ((X_val_A,X_val_B ) , (y_val,y_val) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5',save_best_only = True)\n",
    "\n",
    "input_A = keras.layers.Input(shape = [5], name = 'Wide_Input')\n",
    "input_B = keras.layers.Input(shape = [6], name = 'Deep_Input')\n",
    "\n",
    "hidden_1 = keras.layers.Dense(30,activation = 'relu')(input_B)\n",
    "hidden_2 = keras.layers.Dense(30,activation = 'relu')(hidden_1)\n",
    "\n",
    "concat = keras.layers.Concatenate()([input_A, hidden_2])\n",
    "\n",
    "out1 = keras.layers.Dense(1,name='main_output')(concat)\n",
    "out2 = keras.layers.Dense(1,name='aux_output')(hidden_2)\n",
    "\n",
    "model = keras.Model(inputs= [input_A, input_B],outputs = [out1,out2])    \n",
    "\n",
    "\n",
    "model.compile(loss = 'mean_squared_error',loss_weights = [.9,.1], optimizer = 'sgd')\n",
    "history = model.fit((X_train_A,X_train_B),(y_train,y_train), epochs = 20, validation_data = ((X_val_A,X_val_B ) , (y_val,y_val) ) , callbacks = [checkpoint_cb] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir,'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m-%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5], name = 'Wide_Input')\n",
    "input_B = keras.layers.Input(shape = [6], name = 'Deep_Input')\n",
    "\n",
    "hidden_1 = keras.layers.Dense(30,activation = 'relu')(input_B)\n",
    "hidden_2 = keras.layers.Dense(30,activation = 'relu')(hidden_1)\n",
    "\n",
    "concat = keras.layers.Concatenate()([input_A, hidden_2])\n",
    "\n",
    "out1 = keras.layers.Dense(1,name='main_output')(concat)\n",
    "out2 = keras.layers.Dense(1,name='aux_output')(hidden_2)\n",
    "\n",
    "model = keras.Model(inputs= [input_A, input_B],outputs = [out1,out2])    \n",
    "\n",
    "\n",
    "model.compile(loss = 'mean_squared_error',loss_weights = [.9,.1], optimizer = 'sgd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "history = model.fit((X_train_A,X_train_B),(y_train,y_train), epochs = 20, validation_data = ((X_val_A,X_val_B ) , (y_val,y_val) ), callbacks=  [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Model(inputs= [input_A, input_B],outputs = [out1,out2])    \n",
    "model2.compile(loss = 'mean_squared_error',loss_weights = [.9,.1], optimizer = keras.optimizers.SGD(learning_rate = .5e-3))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "history = model2.fit((X_train_A,X_train_B),(y_train,y_train), epochs = 20, validation_data = ((X_val_A,X_val_B ) , (y_val,y_val) ), callbacks=  [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tensorflow.summary.create_file_writer(test_logdir)\n",
    "\n",
    "with writer.as_default():\n",
    "    for step in range(1,1000+1):\n",
    "        tensorflow.summary.scalar('my_scalar', np.sin(step/ 10), step = step)\n",
    "        data = (np.random.randn(100) + 2) * step/ 100 #random data\n",
    "        tensorflow.summary.histogram('my_hist',data , buckets = 50,step = step)\n",
    "        images = np.random.rand(2,32,32,3) #random 32 x 32\n",
    "        tensorflow.summary.image('my_images',images * step /1000 , step = step)\n",
    "        texts = ['the step is ' + str(step), 'Its square is ' + str(step**2)]\n",
    "        tensorflow.summary.text('my_text',texts,step = step)\n",
    "        sine_wave = tensorflow.math.sin(tensorflow.range(12000) / 48000 * 2*np.pi*step)\n",
    "        audio = tensorflow.reshape(tensorflow.cast(sine_wave,tensorflow.float32),[1,-1,1])\n",
    "        tensorflow.summary.audio('my_audio',audio,sample_rate=48000,step= step)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping SciKit around Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a function that wraps around Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden =1 , n_neurons = 30, learning_rate = 3e-3, input_shape = [8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape = input_shape))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons,activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate = learning_rate)\n",
    "    model.compile(loss= 'mse',optimizer = optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New Wrapper - Cannot run grid search on it, the parameters are not accepted by the \n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "my_keras_reg = KerasRegressor(build_model,n_hidden =1 , n_neurons = 30, learning_rate = 3e-3)\n",
    "\n",
    "my_keras_reg.fit(X_train,y_train, epochs = 20 , validation_data = (X_val,y_val),callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
    "\n",
    "\n",
    "y_pred = my_keras_reg.predict(X_val)\n",
    "print(y_pred)\n",
    "print(my_keras_reg.score(X_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following the book, depreciated method\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "keras_reg.fit(X_train,y_train,epochs = 20, validation_data = (X_val,y_val),callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
    "\n",
    "y_pred = keras_reg.predict(X_test[:3])\n",
    "\n",
    "print(y_pred)\n",
    "print(keras_reg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_distribs = {\n",
    "    'n_hidden' : range(4),\n",
    "    'n_neurons': np.arange(1,100),\n",
    "    'learning_rate': reciprocal(3e-4,3e-2)\n",
    "}\n",
    "\n",
    "ran_search_cv = RandomizedSearchCV(keras_reg,param_distribs,n_iter =10, cv = 3, verbose =4)\n",
    "ran_search_cv.fit(X_train,y_train,epochs = 100, validation_data = (X_val, y_val), callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ran_search_cv.best_estimator_.model\n",
    "\n",
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    'n_hidden' : range(4),\n",
    "    'n_neurons': np.arange(1,100),\n",
    "    'learning_rate': reciprocal(3e-4,3e-2)\n",
    "}\n",
    "\n",
    "ran_search_cv = RandomizedSearchCV(my_keras_reg,param_distribs,n_iter = 10, cv = 3, verbose =4)\n",
    "ran_search_cv.fit(X_train,y_train,epochs = 100, validation_data = (X_val, y_val), callbacks = [keras.callbacks.EarlyStopping(patience = 10), keras.callbacks.ModelCheckpoint('new_my_keras_model.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_search_cv.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ran_search_cv.best_estimator_.model()\n",
    "\n",
    "model.save('new_my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = keras.models.load_model('new_my_keras_model.h5')\n",
    "m2 = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = m1.predict(X_val)\n",
    "print(r2_score(y_pred,y_val))\n",
    "\n",
    "plt.scatter(m1.predict(X_test),y_test,color='blue',marker='o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_pred = m2.predict(X_val)\n",
    "print(r2_score(y_pred,y_val))\n",
    "\n",
    "plt.scatter(m2.predict(X_test),y_test,color='blue',marker='o'); plt.xlim([0,8]); plt.ylim([0,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_val)\n",
    "print(r2_score(y_pred,y_val))\n",
    "\n",
    "plt.scatter(lr.predict(X_test),y_test,color='blue',marker='o'); plt.xlim([0,8]); plt.ylim([0,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir,'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m-%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full,y_train_full), (X_test,y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train_full,y_train_full,test_size = .2)\n",
    "\n",
    "X_val , X_test, X_train = X_val/255. , X_test/255., X_train/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch,lr):\n",
    "    lrs = 10**np.linspace(-7,0,100,dtype = np.float32)\n",
    "    try:\n",
    "        return lrs[epoch]\n",
    "    except ERR:\n",
    "        return lr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('mnist_model2.h5',monitor = 'val_accuracy',verbose = 1,save_best_only = True)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor = 'val_accuracy',verbose = 1, patience = 20,restore_best_weights = True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "lr_cb = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.0000000116860974e-07.\n",
      "Epoch 1/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 2.4380 - accuracy: 0.0793\n",
      "Epoch 1: val_accuracy improved from -inf to 0.07908, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 2.4378 - accuracy: 0.0794 - val_loss: 2.4373 - val_accuracy: 0.0791 - lr: 1.0000e-07\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 1.1768113239440936e-07.\n",
      "Epoch 2/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 2.4376 - accuracy: 0.0794\n",
      "Epoch 2: val_accuracy improved from 0.07908 to 0.07917, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4377 - accuracy: 0.0794 - val_loss: 2.4372 - val_accuracy: 0.0792 - lr: 1.1768e-07\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 1.3848864455212606e-07.\n",
      "Epoch 3/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 2.4377 - accuracy: 0.0793\n",
      "Epoch 3: val_accuracy did not improve from 0.07917\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4375 - accuracy: 0.0794 - val_loss: 2.4370 - val_accuracy: 0.0792 - lr: 1.3849e-07\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 1.6297501304052275e-07.\n",
      "Epoch 4/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 2.4370 - accuracy: 0.0793\n",
      "Epoch 4: val_accuracy did not improve from 0.07917\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4373 - accuracy: 0.0794 - val_loss: 2.4367 - val_accuracy: 0.0792 - lr: 1.6298e-07\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 1.9179104526756419e-07.\n",
      "Epoch 5/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 2.4370 - accuracy: 0.0794\n",
      "Epoch 5: val_accuracy did not improve from 0.07917\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4370 - accuracy: 0.0794 - val_loss: 2.4364 - val_accuracy: 0.0792 - lr: 1.9179e-07\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 2.2570188207282627e-07.\n",
      "Epoch 6/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 2.4366 - accuracy: 0.0794\n",
      "Epoch 6: val_accuracy improved from 0.07917 to 0.07925, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4366 - accuracy: 0.0794 - val_loss: 2.4360 - val_accuracy: 0.0793 - lr: 2.2570e-07\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 2.6560883270576596e-07.\n",
      "Epoch 7/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 2.4361 - accuracy: 0.0795\n",
      "Epoch 7: val_accuracy improved from 0.07925 to 0.07933, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4362 - accuracy: 0.0795 - val_loss: 2.4355 - val_accuracy: 0.0793 - lr: 2.6561e-07\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 3.1257147270480345e-07.\n",
      "Epoch 8/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 2.4357 - accuracy: 0.0796\n",
      "Epoch 8: val_accuracy did not improve from 0.07933\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4356 - accuracy: 0.0796 - val_loss: 2.4349 - val_accuracy: 0.0793 - lr: 3.1257e-07\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 3.6783805512641266e-07.\n",
      "Epoch 9/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 2.4350 - accuracy: 0.0797\n",
      "Epoch 9: val_accuracy improved from 0.07933 to 0.07983, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4350 - accuracy: 0.0797 - val_loss: 2.4342 - val_accuracy: 0.0798 - lr: 3.6784e-07\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 4.328759928284853e-07.\n",
      "Epoch 10/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 2.4341 - accuracy: 0.0798\n",
      "Epoch 10: val_accuracy did not improve from 0.07983\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4342 - accuracy: 0.0798 - val_loss: 2.4333 - val_accuracy: 0.0798 - lr: 4.3288e-07\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 5.094139510219975e-07.\n",
      "Epoch 11/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 2.4331 - accuracy: 0.0799\n",
      "Epoch 11: val_accuracy did not improve from 0.07983\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4332 - accuracy: 0.0799 - val_loss: 2.4322 - val_accuracy: 0.0798 - lr: 5.0941e-07\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 5.994841103529325e-07.\n",
      "Epoch 12/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 2.4322 - accuracy: 0.0802\n",
      "Epoch 12: val_accuracy improved from 0.07983 to 0.08000, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4321 - accuracy: 0.0801 - val_loss: 2.4310 - val_accuracy: 0.0800 - lr: 5.9948e-07\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 7.054804882500321e-07.\n",
      "Epoch 13/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 2.4307 - accuracy: 0.0803\n",
      "Epoch 13: val_accuracy improved from 0.08000 to 0.08025, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4307 - accuracy: 0.0803 - val_loss: 2.4295 - val_accuracy: 0.0803 - lr: 7.0548e-07\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 8.302174023810949e-07.\n",
      "Epoch 14/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 2.4291 - accuracy: 0.0808\n",
      "Epoch 14: val_accuracy improved from 0.08025 to 0.08050, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 2.4291 - accuracy: 0.0807 - val_loss: 2.4278 - val_accuracy: 0.0805 - lr: 8.3022e-07\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 9.770103588380152e-07.\n",
      "Epoch 15/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 2.4278 - accuracy: 0.0808\n",
      "Epoch 15: val_accuracy improved from 0.08050 to 0.08067, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.4273 - accuracy: 0.0812 - val_loss: 2.4258 - val_accuracy: 0.0807 - lr: 9.7701e-07\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 1.1497568266349845e-06.\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 2.4251 - accuracy: 0.0814\n",
      "Epoch 16: val_accuracy improved from 0.08067 to 0.08092, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4251 - accuracy: 0.0814 - val_loss: 2.4235 - val_accuracy: 0.0809 - lr: 1.1498e-06\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 1.3530483329304843e-06.\n",
      "Epoch 17/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 2.4225 - accuracy: 0.0819\n",
      "Epoch 17: val_accuracy improved from 0.08092 to 0.08125, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4225 - accuracy: 0.0819 - val_loss: 2.4207 - val_accuracy: 0.0812 - lr: 1.3530e-06\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 1.5922826150926994e-06.\n",
      "Epoch 18/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 2.4196 - accuracy: 0.0826\n",
      "Epoch 18: val_accuracy improved from 0.08125 to 0.08158, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4196 - accuracy: 0.0825 - val_loss: 2.4176 - val_accuracy: 0.0816 - lr: 1.5923e-06\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 1.8738184053290752e-06.\n",
      "Epoch 19/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 2.4162 - accuracy: 0.0833\n",
      "Epoch 19: val_accuracy improved from 0.08158 to 0.08250, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4162 - accuracy: 0.0832 - val_loss: 2.4139 - val_accuracy: 0.0825 - lr: 1.8738e-06\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 2.205130613219808e-06.\n",
      "Epoch 20/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 2.4121 - accuracy: 0.0839\n",
      "Epoch 20: val_accuracy improved from 0.08250 to 0.08317, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4122 - accuracy: 0.0838 - val_loss: 2.4096 - val_accuracy: 0.0832 - lr: 2.2051e-06\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 2.5950228064175462e-06.\n",
      "Epoch 21/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 2.4080 - accuracy: 0.0844\n",
      "Epoch 21: val_accuracy improved from 0.08317 to 0.08383, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4076 - accuracy: 0.0847 - val_loss: 2.4047 - val_accuracy: 0.0838 - lr: 2.5950e-06\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 3.0538556075043743e-06.\n",
      "Epoch 22/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 2.4024 - accuracy: 0.0858\n",
      "Epoch 22: val_accuracy improved from 0.08383 to 0.08550, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4023 - accuracy: 0.0859 - val_loss: 2.3990 - val_accuracy: 0.0855 - lr: 3.0539e-06\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 3.593811925384216e-06.\n",
      "Epoch 23/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 2.3964 - accuracy: 0.0870\n",
      "Epoch 23: val_accuracy improved from 0.08550 to 0.08775, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.3962 - accuracy: 0.0872 - val_loss: 2.3925 - val_accuracy: 0.0878 - lr: 3.5938e-06\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 4.229243131703697e-06.\n",
      "Epoch 24/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 2.3889 - accuracy: 0.0889\n",
      "Epoch 24: val_accuracy improved from 0.08775 to 0.08908, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.3892 - accuracy: 0.0889 - val_loss: 2.3851 - val_accuracy: 0.0891 - lr: 4.2292e-06\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 4.977021490049083e-06.\n",
      "Epoch 25/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 2.3813 - accuracy: 0.0905\n",
      "Epoch 25: val_accuracy improved from 0.08908 to 0.09058, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.3812 - accuracy: 0.0905 - val_loss: 2.3766 - val_accuracy: 0.0906 - lr: 4.9770e-06\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 5.857021733390866e-06.\n",
      "Epoch 26/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 2.3723 - accuracy: 0.0920\n",
      "Epoch 26: val_accuracy improved from 0.09058 to 0.09275, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.3722 - accuracy: 0.0919 - val_loss: 2.3669 - val_accuracy: 0.0927 - lr: 5.8570e-06\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 6.892609690112295e-06.\n",
      "Epoch 27/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 2.3621 - accuracy: 0.0940\n",
      "Epoch 27: val_accuracy improved from 0.09275 to 0.09517, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.3619 - accuracy: 0.0941 - val_loss: 2.3560 - val_accuracy: 0.0952 - lr: 6.8926e-06\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 8.111310307867825e-06.\n",
      "Epoch 28/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 2.3503 - accuracy: 0.0967\n",
      "Epoch 28: val_accuracy improved from 0.09517 to 0.09742, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3502 - accuracy: 0.0968 - val_loss: 2.3437 - val_accuracy: 0.0974 - lr: 8.1113e-06\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 9.545481589157134e-06.\n",
      "Epoch 29/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 2.3372 - accuracy: 0.1009\n",
      "Epoch 29: val_accuracy improved from 0.09742 to 0.10042, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3371 - accuracy: 0.1010 - val_loss: 2.3299 - val_accuracy: 0.1004 - lr: 9.5455e-06\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 1.1233243640162982e-05.\n",
      "Epoch 30/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 2.3226 - accuracy: 0.1051\n",
      "Epoch 30: val_accuracy improved from 0.10042 to 0.10542, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 2.3224 - accuracy: 0.1051 - val_loss: 2.3144 - val_accuracy: 0.1054 - lr: 1.1233e-05\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1.3219408174336422e-05.\n",
      "Epoch 31/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 2.3061 - accuracy: 0.1102\n",
      "Epoch 31: val_accuracy improved from 0.10542 to 0.11125, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 2.3060 - accuracy: 0.1102 - val_loss: 2.2971 - val_accuracy: 0.1112 - lr: 1.3219e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1.5556766811641864e-05.\n",
      "Epoch 32/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 2.2879 - accuracy: 0.1166\n",
      "Epoch 32: val_accuracy improved from 0.11125 to 0.11892, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.2876 - accuracy: 0.1166 - val_loss: 2.2777 - val_accuracy: 0.1189 - lr: 1.5557e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1.8307378923054785e-05.\n",
      "Epoch 33/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 2.2672 - accuracy: 0.1255\n",
      "Epoch 33: val_accuracy improved from 0.11892 to 0.12892, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.2670 - accuracy: 0.1255 - val_loss: 2.2561 - val_accuracy: 0.1289 - lr: 1.8307e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 2.15443542401772e-05.\n",
      "Epoch 34/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 2.2441 - accuracy: 0.1389\n",
      "Epoch 34: val_accuracy improved from 0.12892 to 0.14367, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 2.2440 - accuracy: 0.1389 - val_loss: 2.2318 - val_accuracy: 0.1437 - lr: 2.1544e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 2.5353641831316054e-05.\n",
      "Epoch 35/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 2.2182 - accuracy: 0.1574\n",
      "Epoch 35: val_accuracy improved from 0.14367 to 0.16675, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.2181 - accuracy: 0.1575 - val_loss: 2.2044 - val_accuracy: 0.1667 - lr: 2.5354e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 2.9836484827683307e-05.\n",
      "Epoch 36/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 2.1888 - accuracy: 0.1888\n",
      "Epoch 36: val_accuracy improved from 0.16675 to 0.20508, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.1888 - accuracy: 0.1888 - val_loss: 2.1733 - val_accuracy: 0.2051 - lr: 2.9836e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 3.511191243887879e-05.\n",
      "Epoch 37/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 2.1555 - accuracy: 0.2329\n",
      "Epoch 37: val_accuracy improved from 0.20508 to 0.25750, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.1555 - accuracy: 0.2328 - val_loss: 2.1378 - val_accuracy: 0.2575 - lr: 3.5112e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 4.132014510105364e-05.\n",
      "Epoch 38/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 2.1174 - accuracy: 0.2897\n",
      "Epoch 38: val_accuracy improved from 0.25750 to 0.31858, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 2.1172 - accuracy: 0.2901 - val_loss: 2.0969 - val_accuracy: 0.3186 - lr: 4.1320e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 4.8626014176988974e-05.\n",
      "Epoch 39/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 2.0730 - accuracy: 0.3581\n",
      "Epoch 39: val_accuracy improved from 0.31858 to 0.39517, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.0729 - accuracy: 0.3582 - val_loss: 2.0491 - val_accuracy: 0.3952 - lr: 4.8626e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 5.722370769944973e-05.\n",
      "Epoch 40/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 2.0210 - accuracy: 0.4323\n",
      "Epoch 40: val_accuracy improved from 0.39517 to 0.46817, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.0208 - accuracy: 0.4323 - val_loss: 1.9929 - val_accuracy: 0.4682 - lr: 5.7224e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 6.734150520060211e-05.\n",
      "Epoch 41/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.9594 - accuracy: 0.5022\n",
      "Epoch 41: val_accuracy improved from 0.46817 to 0.53708, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.9592 - accuracy: 0.5024 - val_loss: 1.9260 - val_accuracy: 0.5371 - lr: 6.7342e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 7.924824603833258e-05.\n",
      "Epoch 42/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 1.8856 - accuracy: 0.5679\n",
      "Epoch 42: val_accuracy improved from 0.53708 to 0.60133, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.8855 - accuracy: 0.5681 - val_loss: 1.8459 - val_accuracy: 0.6013 - lr: 7.9248e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 9.326034341938794e-05.\n",
      "Epoch 43/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 1.7974 - accuracy: 0.6235\n",
      "Epoch 43: val_accuracy improved from 0.60133 to 0.65467, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.7974 - accuracy: 0.6234 - val_loss: 1.7501 - val_accuracy: 0.6547 - lr: 9.3260e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.00010974988981615752.\n",
      "Epoch 44/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 1.6925 - accuracy: 0.6751\n",
      "Epoch 44: val_accuracy improved from 0.65467 to 0.70158, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.6923 - accuracy: 0.6754 - val_loss: 1.6364 - val_accuracy: 0.7016 - lr: 1.0975e-04\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.00012915498518850654.\n",
      "Epoch 45/100\n",
      "1480/1500 [============================>.] - ETA: 0s - loss: 1.5702 - accuracy: 0.7166\n",
      "Epoch 45: val_accuracy improved from 0.70158 to 0.73558, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5694 - accuracy: 0.7167 - val_loss: 1.5054 - val_accuracy: 0.7356 - lr: 1.2915e-04\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.0001519911311333999.\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.4309 - accuracy: 0.7485\n",
      "Epoch 46: val_accuracy improved from 0.73558 to 0.76100, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 1.4309 - accuracy: 0.7485 - val_loss: 1.3608 - val_accuracy: 0.7610 - lr: 1.5199e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.00017886498244479299.\n",
      "Epoch 47/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 1.2829 - accuracy: 0.7720\n",
      "Epoch 47: val_accuracy improved from 0.76100 to 0.78467, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 1.2822 - accuracy: 0.7722 - val_loss: 1.2097 - val_accuracy: 0.7847 - lr: 1.7886e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.00021049045608378947.\n",
      "Epoch 48/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 1.1326 - accuracy: 0.7927\n",
      "Epoch 48: val_accuracy improved from 0.78467 to 0.80108, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.1316 - accuracy: 0.7928 - val_loss: 1.0612 - val_accuracy: 0.8011 - lr: 2.1049e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.0002477077068760991.\n",
      "Epoch 49/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.9890 - accuracy: 0.8111\n",
      "Epoch 49: val_accuracy improved from 0.80108 to 0.81800, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.9888 - accuracy: 0.8111 - val_loss: 0.9249 - val_accuracy: 0.8180 - lr: 2.4771e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.00029150539194233716.\n",
      "Epoch 50/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 0.8615 - accuracy: 0.8257\n",
      "Epoch 50: val_accuracy improved from 0.81800 to 0.82908, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8616 - accuracy: 0.8256 - val_loss: 0.8071 - val_accuracy: 0.8291 - lr: 2.9151e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.00034304684959352016.\n",
      "Epoch 51/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.7542 - accuracy: 0.8382\n",
      "Epoch 51: val_accuracy improved from 0.82908 to 0.84108, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.7542 - accuracy: 0.8383 - val_loss: 0.7088 - val_accuracy: 0.8411 - lr: 3.4305e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.00040370161877945065.\n",
      "Epoch 52/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.6659 - accuracy: 0.8481\n",
      "Epoch 52: val_accuracy improved from 0.84108 to 0.85150, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.6661 - accuracy: 0.8480 - val_loss: 0.6290 - val_accuracy: 0.8515 - lr: 4.0370e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.00047508091665804386.\n",
      "Epoch 53/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 0.5954 - accuracy: 0.8581\n",
      "Epoch 53: val_accuracy improved from 0.85150 to 0.86225, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.5949 - accuracy: 0.8583 - val_loss: 0.5647 - val_accuracy: 0.8622 - lr: 4.7508e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.0005590809159912169.\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.8663\n",
      "Epoch 54: val_accuracy improved from 0.86225 to 0.87017, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.5374 - accuracy: 0.8663 - val_loss: 0.5121 - val_accuracy: 0.8702 - lr: 5.5908e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.0006579331238754094.\n",
      "Epoch 55/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.4906 - accuracy: 0.8744\n",
      "Epoch 55: val_accuracy improved from 0.87017 to 0.87758, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4907 - accuracy: 0.8743 - val_loss: 0.4695 - val_accuracy: 0.8776 - lr: 6.5793e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0007742635789327323.\n",
      "Epoch 56/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 0.4530 - accuracy: 0.8808\n",
      "Epoch 56: val_accuracy improved from 0.87758 to 0.88342, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4524 - accuracy: 0.8809 - val_loss: 0.4346 - val_accuracy: 0.8834 - lr: 7.7426e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.0009111626422964036.\n",
      "Epoch 57/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8872\n",
      "Epoch 57: val_accuracy improved from 0.88342 to 0.89033, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4208 - accuracy: 0.8873 - val_loss: 0.4045 - val_accuracy: 0.8903 - lr: 9.1116e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0010722671868279576.\n",
      "Epoch 58/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8921\n",
      "Epoch 58: val_accuracy improved from 0.89033 to 0.89508, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3940 - accuracy: 0.8922 - val_loss: 0.3797 - val_accuracy: 0.8951 - lr: 0.0011\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.001261856872588396.\n",
      "Epoch 59/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 0.3712 - accuracy: 0.8960\n",
      "Epoch 59: val_accuracy improved from 0.89508 to 0.90092, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.3711 - accuracy: 0.8961 - val_loss: 0.3579 - val_accuracy: 0.9009 - lr: 0.0013\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0014849682338535786.\n",
      "Epoch 60/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 0.3505 - accuracy: 0.9015\n",
      "Epoch 60: val_accuracy improved from 0.90092 to 0.90592, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3505 - accuracy: 0.9016 - val_loss: 0.3385 - val_accuracy: 0.9059 - lr: 0.0015\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.0017475284403190017.\n",
      "Epoch 61/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9065\n",
      "Epoch 61: val_accuracy improved from 0.90592 to 0.90992, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3319 - accuracy: 0.9068 - val_loss: 0.3210 - val_accuracy: 0.9099 - lr: 0.0017\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0020565122831612825.\n",
      "Epoch 62/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.9115\n",
      "Epoch 62: val_accuracy improved from 0.90992 to 0.91350, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3144 - accuracy: 0.9113 - val_loss: 0.3056 - val_accuracy: 0.9135 - lr: 0.0021\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.00242012832313776.\n",
      "Epoch 63/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.9153\n",
      "Epoch 63: val_accuracy improved from 0.91350 to 0.91558, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2982 - accuracy: 0.9153 - val_loss: 0.2901 - val_accuracy: 0.9156 - lr: 0.0024\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.0028480361215770245.\n",
      "Epoch 64/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.9197\n",
      "Epoch 64: val_accuracy improved from 0.91558 to 0.92067, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2825 - accuracy: 0.9198 - val_loss: 0.2742 - val_accuracy: 0.9207 - lr: 0.0028\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.0033516031689941883.\n",
      "Epoch 65/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9240\n",
      "Epoch 65: val_accuracy improved from 0.92067 to 0.92450, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2668 - accuracy: 0.9240 - val_loss: 0.2597 - val_accuracy: 0.9245 - lr: 0.0034\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.003944206517189741.\n",
      "Epoch 66/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9278\n",
      "Epoch 66: val_accuracy improved from 0.92450 to 0.92775, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2512 - accuracy: 0.9278 - val_loss: 0.2474 - val_accuracy: 0.9277 - lr: 0.0039\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.00464158970862627.\n",
      "Epoch 67/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9329\n",
      "Epoch 67: val_accuracy improved from 0.92775 to 0.93400, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2360 - accuracy: 0.9328 - val_loss: 0.2310 - val_accuracy: 0.9340 - lr: 0.0046\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.00546227814629674.\n",
      "Epoch 68/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.9381\n",
      "Epoch 68: val_accuracy improved from 0.93400 to 0.93725, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2199 - accuracy: 0.9380 - val_loss: 0.2187 - val_accuracy: 0.9373 - lr: 0.0055\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.006428074557334185.\n",
      "Epoch 69/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.2045 - accuracy: 0.9414\n",
      "Epoch 69: val_accuracy improved from 0.93725 to 0.94183, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2043 - accuracy: 0.9414 - val_loss: 0.2022 - val_accuracy: 0.9418 - lr: 0.0064\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0075646354816854.\n",
      "Epoch 70/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9462\n",
      "Epoch 70: val_accuracy improved from 0.94183 to 0.94625, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1879 - accuracy: 0.9463 - val_loss: 0.1874 - val_accuracy: 0.9463 - lr: 0.0076\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.008902148343622684.\n",
      "Epoch 71/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9511\n",
      "Epoch 71: val_accuracy improved from 0.94625 to 0.95100, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1718 - accuracy: 0.9511 - val_loss: 0.1760 - val_accuracy: 0.9510 - lr: 0.0089\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.010476158000528812.\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9553\n",
      "Epoch 72: val_accuracy improved from 0.95100 to 0.95450, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1561 - accuracy: 0.9553 - val_loss: 0.1582 - val_accuracy: 0.9545 - lr: 0.0105\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.012328468263149261.\n",
      "Epoch 73/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9599\n",
      "Epoch 73: val_accuracy improved from 0.95450 to 0.95883, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1404 - accuracy: 0.9601 - val_loss: 0.1474 - val_accuracy: 0.9588 - lr: 0.0123\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.014508289285004139.\n",
      "Epoch 74/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9644\n",
      "Epoch 74: val_accuracy improved from 0.95883 to 0.96092, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1261 - accuracy: 0.9644 - val_loss: 0.1345 - val_accuracy: 0.9609 - lr: 0.0145\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.017073528841137886.\n",
      "Epoch 75/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 0.1130 - accuracy: 0.9680\n",
      "Epoch 75: val_accuracy improved from 0.96092 to 0.96300, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1130 - accuracy: 0.9680 - val_loss: 0.1227 - val_accuracy: 0.9630 - lr: 0.0171\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.020092327147722244.\n",
      "Epoch 76/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9711\n",
      "Epoch 76: val_accuracy improved from 0.96300 to 0.96583, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0998 - accuracy: 0.9711 - val_loss: 0.1178 - val_accuracy: 0.9658 - lr: 0.0201\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.023644892498850822.\n",
      "Epoch 77/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9743\n",
      "Epoch 77: val_accuracy improved from 0.96583 to 0.96683, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0890 - accuracy: 0.9742 - val_loss: 0.1124 - val_accuracy: 0.9668 - lr: 0.0236\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.02782559208571911.\n",
      "Epoch 78/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9774\n",
      "Epoch 78: val_accuracy improved from 0.96683 to 0.96717, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0788 - accuracy: 0.9772 - val_loss: 0.1110 - val_accuracy: 0.9672 - lr: 0.0278\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.032745491713285446.\n",
      "Epoch 79/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9799\n",
      "Epoch 79: val_accuracy improved from 0.96717 to 0.97217, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0697 - accuracy: 0.9799 - val_loss: 0.0933 - val_accuracy: 0.9722 - lr: 0.0327\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.038535285741090775.\n",
      "Epoch 80/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9824\n",
      "Epoch 80: val_accuracy improved from 0.97217 to 0.97533, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.0614 - accuracy: 0.9825 - val_loss: 0.0889 - val_accuracy: 0.9753 - lr: 0.0385\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 0.04534878581762314.\n",
      "Epoch 81/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 0.0550 - accuracy: 0.9834\n",
      "Epoch 81: val_accuracy did not improve from 0.97533\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0549 - accuracy: 0.9834 - val_loss: 0.0901 - val_accuracy: 0.9728 - lr: 0.0453\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.053366996347904205.\n",
      "Epoch 82/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.0496 - accuracy: 0.9846\n",
      "Epoch 82: val_accuracy did not improve from 0.97533\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0499 - accuracy: 0.9845 - val_loss: 0.0963 - val_accuracy: 0.9708 - lr: 0.0534\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 0.06280291825532913.\n",
      "Epoch 83/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9859\n",
      "Epoch 83: val_accuracy did not improve from 0.97533\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0454 - accuracy: 0.9859 - val_loss: 0.0991 - val_accuracy: 0.9712 - lr: 0.0628\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 0.07390722632408142.\n",
      "Epoch 84/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9863\n",
      "Epoch 84: val_accuracy did not improve from 0.97533\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0434 - accuracy: 0.9862 - val_loss: 0.1030 - val_accuracy: 0.9698 - lr: 0.0739\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 0.08697491139173508.\n",
      "Epoch 85/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 0.9867\n",
      "Epoch 85: val_accuracy did not improve from 0.97533\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0409 - accuracy: 0.9866 - val_loss: 0.1040 - val_accuracy: 0.9693 - lr: 0.0870\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 0.10235310345888138.\n",
      "Epoch 86/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9873\n",
      "Epoch 86: val_accuracy did not improve from 0.97533\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0400 - accuracy: 0.9872 - val_loss: 0.0995 - val_accuracy: 0.9723 - lr: 0.1024\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 0.12045036256313324.\n",
      "Epoch 87/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 0.0445 - accuracy: 0.9859\n",
      "Epoch 87: val_accuracy improved from 0.97533 to 0.97608, saving model to mnist_model2.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0446 - accuracy: 0.9859 - val_loss: 0.0865 - val_accuracy: 0.9761 - lr: 0.1205\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 0.14174741506576538.\n",
      "Epoch 88/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9864\n",
      "Epoch 88: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0411 - accuracy: 0.9865 - val_loss: 0.1002 - val_accuracy: 0.9724 - lr: 0.1417\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 0.1668100506067276.\n",
      "Epoch 89/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 0.9844\n",
      "Epoch 89: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0467 - accuracy: 0.9844 - val_loss: 0.1144 - val_accuracy: 0.9683 - lr: 0.1668\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 0.1963040679693222.\n",
      "Epoch 90/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.0480 - accuracy: 0.9845\n",
      "Epoch 90: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0481 - accuracy: 0.9844 - val_loss: 0.1264 - val_accuracy: 0.9662 - lr: 0.1963\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 0.2310129702091217.\n",
      "Epoch 91/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9842\n",
      "Epoch 91: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0487 - accuracy: 0.9843 - val_loss: 0.1281 - val_accuracy: 0.9667 - lr: 0.2310\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 0.2718588411808014.\n",
      "Epoch 92/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.0509 - accuracy: 0.9834\n",
      "Epoch 92: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0510 - accuracy: 0.9833 - val_loss: 0.1448 - val_accuracy: 0.9638 - lr: 0.2719\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 0.3199267089366913.\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9812\n",
      "Epoch 93: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0583 - accuracy: 0.9812 - val_loss: 0.1245 - val_accuracy: 0.9704 - lr: 0.3199\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 0.37649357318878174.\n",
      "Epoch 94/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9815\n",
      "Epoch 94: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0596 - accuracy: 0.9815 - val_loss: 0.1077 - val_accuracy: 0.9744 - lr: 0.3765\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 0.4430621564388275.\n",
      "Epoch 95/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9806\n",
      "Epoch 95: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0624 - accuracy: 0.9806 - val_loss: 0.1360 - val_accuracy: 0.9703 - lr: 0.4431\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 0.5214008688926697.\n",
      "Epoch 96/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9773\n",
      "Epoch 96: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0801 - accuracy: 0.9772 - val_loss: 0.1349 - val_accuracy: 0.9681 - lr: 0.5214\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 0.6135907173156738.\n",
      "Epoch 97/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 0.0926 - accuracy: 0.9736\n",
      "Epoch 97: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0927 - accuracy: 0.9736 - val_loss: 0.1387 - val_accuracy: 0.9657 - lr: 0.6136\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 0.7220808863639832.\n",
      "Epoch 98/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9648\n",
      "Epoch 98: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1363 - accuracy: 0.9647 - val_loss: 0.2126 - val_accuracy: 0.9526 - lr: 0.7221\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 0.8497534394264221.\n",
      "Epoch 99/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9587\n",
      "Epoch 99: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1667 - accuracy: 0.9588 - val_loss: 0.1866 - val_accuracy: 0.9581 - lr: 0.8498\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 1.0.\n",
      "Epoch 100/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.2163\n",
      "Epoch 100: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: nan - accuracy: 0.2152 - val_loss: nan - val_accuracy: 0.0979 - lr: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = (28,28), name = 'in_flat'),\n",
    "    keras.layers.Dense(300,activation = 'relu'),\n",
    "    keras.layers.Dense(100,activation = 'relu'),\n",
    "    keras.layers.Dense(10,activation = 'softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy',optimizer = keras.optimizers.SGD(learning_rate = 1e-3),metrics = ['accuracy'])\n",
    "history = model.fit(X_train,y_train, epochs = 100, validation_data = (X_val,y_val),\n",
    "         callbacks = [checkpoint_cb,earlystop_cb,tensorboard_cb,lr_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='lr'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_,ax = plt.subplots(ncols =2 , figsize = (20,3))\n",
    "hist_df.plot(x = 'lr',y = ['loss','val_loss'],ax=ax[0],marker='o',logx = True)\n",
    "hist_df.plot(x = 'lr',y = ['accuracy','val_accuracy'],ax=ax[1],marker='o', logx = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss        85\n",
      "val_loss    86\n",
      "dtype: int64\n",
      "accuracy        85\n",
      "val_accuracy    86\n",
      "dtype: int64\n",
      "85\n",
      "0.1023531\n"
     ]
    }
   ],
   "source": [
    "print(hist_df[['loss','val_loss']].idxmin())\n",
    "print(hist_df[['accuracy','val_accuracy']].idxmax())\n",
    "\n",
    "indx = np.mean([hist_df[['loss','val_loss']].idxmin().values,hist_df[['accuracy','val_accuracy']].idxmax().values])\n",
    "print(int(indx))\n",
    "\n",
    "opt_lr = hist_df.loc[int(indx),'lr']\n",
    "print(opt_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x20e981f3640>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_,ax = plt.subplots(ncols =2 , figsize = (20,3))\n",
    "hist_df.plot(x = 'lr',y = ['loss','val_loss'],ax=ax[0],marker='o',logx = True); ax[0].set_xlim([.001, .8])\n",
    "hist_df.plot(x = 'lr',y = ['accuracy','val_accuracy'],ax=ax[1],marker='o', logx = True); ax[1].set_xlim([.001, .8])\n",
    "\n",
    "ax[0].axvline(1e-1,0,2,color = 'r',dashes = (1,1,1,1))\n",
    "ax[0].axvline(3e-3,0,2,color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.9169\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94908, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2789 - accuracy: 0.9169 - val_loss: 0.1682 - val_accuracy: 0.9491\n",
      "Epoch 2/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9650\n",
      "Epoch 2: val_accuracy improved from 0.94908 to 0.96950, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1141 - accuracy: 0.9651 - val_loss: 0.1001 - val_accuracy: 0.9695\n",
      "Epoch 3/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9770\n",
      "Epoch 3: val_accuracy improved from 0.96950 to 0.97350, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0761 - accuracy: 0.9769 - val_loss: 0.0832 - val_accuracy: 0.9735\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9822\n",
      "Epoch 4: val_accuracy improved from 0.97350 to 0.97592, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0580 - accuracy: 0.9822 - val_loss: 0.0786 - val_accuracy: 0.9759\n",
      "Epoch 5/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9876\n",
      "Epoch 5: val_accuracy improved from 0.97592 to 0.97725, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0414 - accuracy: 0.9875 - val_loss: 0.0802 - val_accuracy: 0.9772\n",
      "Epoch 6/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 0.0309 - accuracy: 0.9907\n",
      "Epoch 6: val_accuracy improved from 0.97725 to 0.97750, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0311 - accuracy: 0.9906 - val_loss: 0.0771 - val_accuracy: 0.9775\n",
      "Epoch 7/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9940\n",
      "Epoch 7: val_accuracy improved from 0.97750 to 0.97833, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0222 - accuracy: 0.9940 - val_loss: 0.0755 - val_accuracy: 0.9783\n",
      "Epoch 8/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9956\n",
      "Epoch 8: val_accuracy improved from 0.97833 to 0.97842, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0168 - accuracy: 0.9956 - val_loss: 0.0771 - val_accuracy: 0.9784\n",
      "Epoch 9/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9968\n",
      "Epoch 9: val_accuracy improved from 0.97842 to 0.98025, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0745 - val_accuracy: 0.9803\n",
      "Epoch 10/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9986\n",
      "Epoch 10: val_accuracy improved from 0.98025 to 0.98133, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0076 - accuracy: 0.9986 - val_loss: 0.0738 - val_accuracy: 0.9813\n",
      "Epoch 11/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9990\n",
      "Epoch 11: val_accuracy did not improve from 0.98133\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.0780 - val_accuracy: 0.9798\n",
      "Epoch 12/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9995\n",
      "Epoch 12: val_accuracy did not improve from 0.98133\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0038 - accuracy: 0.9995 - val_loss: 0.1004 - val_accuracy: 0.9747\n",
      "Epoch 13/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9997\n",
      "Epoch 13: val_accuracy did not improve from 0.98133\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.0740 - val_accuracy: 0.9806\n",
      "Epoch 14/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9999\n",
      "Epoch 14: val_accuracy improved from 0.98133 to 0.98200, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0735 - val_accuracy: 0.9820\n",
      "Epoch 15/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 15: val_accuracy did not improve from 0.98200\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0753 - val_accuracy: 0.9817\n",
      "Epoch 16/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 0.98200\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9816\n",
      "Epoch 17/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 9.0932e-04 - accuracy: 1.0000\n",
      "Epoch 17: val_accuracy did not improve from 0.98200\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 9.0875e-04 - accuracy: 1.0000 - val_loss: 0.0768 - val_accuracy: 0.9817\n",
      "Epoch 18/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 7.9149e-04 - accuracy: 1.0000\n",
      "Epoch 18: val_accuracy did not improve from 0.98200\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 7.9617e-04 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9815\n",
      "Epoch 19/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 6.9790e-04 - accuracy: 1.0000\n",
      "Epoch 19: val_accuracy did not improve from 0.98200\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 6.9921e-04 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9818\n",
      "Epoch 20/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 6.5567e-04 - accuracy: 1.0000\n",
      "Epoch 20: val_accuracy did not improve from 0.98200\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 6.5490e-04 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9818\n",
      "Epoch 21/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 5.8581e-04 - accuracy: 1.0000\n",
      "Epoch 21: val_accuracy improved from 0.98200 to 0.98233, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 5.8485e-04 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9823\n",
      "Epoch 22/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 5.4254e-04 - accuracy: 1.0000\n",
      "Epoch 22: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 5.4202e-04 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 0.9819\n",
      "Epoch 23/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 4.9643e-04 - accuracy: 1.0000\n",
      "Epoch 23: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 4.9753e-04 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 0.9820\n",
      "Epoch 24/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 4.6724e-04 - accuracy: 1.0000\n",
      "Epoch 24: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 4.6738e-04 - accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 0.9820\n",
      "Epoch 25/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 4.3199e-04 - accuracy: 1.0000\n",
      "Epoch 25: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 4.3291e-04 - accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 0.9822\n",
      "Epoch 26/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 4.1363e-04 - accuracy: 1.0000\n",
      "Epoch 26: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 4.1260e-04 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9822\n",
      "Epoch 27/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 3.8615e-04 - accuracy: 1.0000\n",
      "Epoch 27: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 3.8729e-04 - accuracy: 1.0000 - val_loss: 0.0828 - val_accuracy: 0.9822\n",
      "Epoch 28/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 3.6333e-04 - accuracy: 1.0000\n",
      "Epoch 28: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 3.6354e-04 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9822\n",
      "Epoch 29/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 3.4854e-04 - accuracy: 1.0000\n",
      "Epoch 29: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 3.4794e-04 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9818\n",
      "Epoch 30/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 3.3093e-04 - accuracy: 1.0000\n",
      "Epoch 30: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 3.3086e-04 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9822\n",
      "Epoch 31/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 3.1408e-04 - accuracy: 1.0000\n",
      "Epoch 31: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 3.1438e-04 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9822\n",
      "Epoch 32/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 3.0100e-04 - accuracy: 1.0000\n",
      "Epoch 32: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 3.0129e-04 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9822\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 2.8776e-04 - accuracy: 1.0000\n",
      "Epoch 33: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.8776e-04 - accuracy: 1.0000 - val_loss: 0.0850 - val_accuracy: 0.9822\n",
      "Epoch 34/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 2.7583e-04 - accuracy: 1.0000\n",
      "Epoch 34: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.7596e-04 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 0.9822\n",
      "Epoch 35/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 2.6636e-04 - accuracy: 1.0000\n",
      "Epoch 35: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.6546e-04 - accuracy: 1.0000 - val_loss: 0.0856 - val_accuracy: 0.9823\n",
      "Epoch 36/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 2.5288e-04 - accuracy: 1.0000\n",
      "Epoch 36: val_accuracy did not improve from 0.98233\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.5317e-04 - accuracy: 1.0000 - val_loss: 0.0862 - val_accuracy: 0.9823\n",
      "Epoch 37/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 2.4490e-04 - accuracy: 1.0000\n",
      "Epoch 37: val_accuracy improved from 0.98233 to 0.98250, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4483e-04 - accuracy: 1.0000 - val_loss: 0.0861 - val_accuracy: 0.9825\n",
      "Epoch 38/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 2.3669e-04 - accuracy: 1.0000\n",
      "Epoch 38: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.3644e-04 - accuracy: 1.0000 - val_loss: 0.0861 - val_accuracy: 0.9820\n",
      "Epoch 39/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 2.2678e-04 - accuracy: 1.0000\n",
      "Epoch 39: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.2743e-04 - accuracy: 1.0000 - val_loss: 0.0866 - val_accuracy: 0.9824\n",
      "Epoch 40/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 2.1936e-04 - accuracy: 1.0000\n",
      "Epoch 40: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.1953e-04 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9824\n",
      "Epoch 41/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 2.1305e-04 - accuracy: 1.0000\n",
      "Epoch 41: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.1237e-04 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 0.9822\n",
      "Epoch 42/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 2.0587e-04 - accuracy: 1.0000\n",
      "Epoch 42: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 2.0588e-04 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 0.9822\n",
      "Epoch 43/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 1.9886e-04 - accuracy: 1.0000\n",
      "Epoch 43: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.9866e-04 - accuracy: 1.0000 - val_loss: 0.0875 - val_accuracy: 0.9825\n",
      "Epoch 44/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 1.9372e-04 - accuracy: 1.0000\n",
      "Epoch 44: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.9375e-04 - accuracy: 1.0000 - val_loss: 0.0876 - val_accuracy: 0.9823\n",
      "Epoch 45/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 1.8756e-04 - accuracy: 1.0000\n",
      "Epoch 45: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.8759e-04 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9822\n",
      "Epoch 46/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 1.8157e-04 - accuracy: 1.0000\n",
      "Epoch 46: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.8149e-04 - accuracy: 1.0000 - val_loss: 0.0885 - val_accuracy: 0.9824\n",
      "Epoch 47/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 1.7685e-04 - accuracy: 1.0000\n",
      "Epoch 47: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.7651e-04 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9821\n",
      "Epoch 48/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 1.7163e-04 - accuracy: 1.0000\n",
      "Epoch 48: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.7152e-04 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9821\n",
      "Epoch 49/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 1.6751e-04 - accuracy: 1.0000\n",
      "Epoch 49: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.6737e-04 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9822\n",
      "Epoch 50/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 1.6318e-04 - accuracy: 1.0000\n",
      "Epoch 50: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.6262e-04 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 0.9820\n",
      "Epoch 51/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.5835e-04 - accuracy: 1.0000\n",
      "Epoch 51: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.5823e-04 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9819\n",
      "Epoch 52/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 1.5411e-04 - accuracy: 1.0000\n",
      "Epoch 52: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.5445e-04 - accuracy: 1.0000 - val_loss: 0.0899 - val_accuracy: 0.9824\n",
      "Epoch 53/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 1.5136e-04 - accuracy: 1.0000\n",
      "Epoch 53: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5094e-04 - accuracy: 1.0000 - val_loss: 0.0900 - val_accuracy: 0.9822\n",
      "Epoch 54/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 1.4708e-04 - accuracy: 1.0000\n",
      "Epoch 54: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.4714e-04 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9822\n",
      "Epoch 55/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 1.4367e-04 - accuracy: 1.0000\n",
      "Epoch 55: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.4370e-04 - accuracy: 1.0000 - val_loss: 0.0905 - val_accuracy: 0.9820\n",
      "Epoch 56/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 1.4031e-04 - accuracy: 1.0000\n",
      "Epoch 56: val_accuracy improved from 0.98250 to 0.98275, saving model to opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.3996e-04 - accuracy: 1.0000 - val_loss: 0.0907 - val_accuracy: 0.9827\n",
      "Epoch 57/100\n",
      "1480/1500 [============================>.] - ETA: 0s - loss: 1.3702e-04 - accuracy: 1.0000\n",
      "Epoch 57: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.3692e-04 - accuracy: 1.0000 - val_loss: 0.0911 - val_accuracy: 0.9824\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.3396e-04 - accuracy: 1.0000\n",
      "Epoch 58: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.3396e-04 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9822\n",
      "Epoch 59/100\n",
      "1491/1500 [============================>.] - ETA: 0s - loss: 1.3124e-04 - accuracy: 1.0000\n",
      "Epoch 59: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.3110e-04 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9823\n",
      "Epoch 60/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.2828e-04 - accuracy: 1.0000\n",
      "Epoch 60: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.2833e-04 - accuracy: 1.0000 - val_loss: 0.0911 - val_accuracy: 0.9821\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.2516e-04 - accuracy: 1.0000\n",
      "Epoch 61: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.2516e-04 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 0.9821\n",
      "Epoch 62/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 1.2251e-04 - accuracy: 1.0000\n",
      "Epoch 62: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.2312e-04 - accuracy: 1.0000 - val_loss: 0.0916 - val_accuracy: 0.9822\n",
      "Epoch 63/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 1.2039e-04 - accuracy: 1.0000\n",
      "Epoch 63: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.2029e-04 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9821\n",
      "Epoch 64/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.1796e-04 - accuracy: 1.0000\n",
      "Epoch 64: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.1798e-04 - accuracy: 1.0000 - val_loss: 0.0917 - val_accuracy: 0.9821\n",
      "Epoch 65/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 1.1551e-04 - accuracy: 1.0000\n",
      "Epoch 65: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.1543e-04 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 0.9823\n",
      "Epoch 66/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 1.1318e-04 - accuracy: 1.0000\n",
      "Epoch 66: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.1349e-04 - accuracy: 1.0000 - val_loss: 0.0922 - val_accuracy: 0.9822\n",
      "Epoch 67/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.1124e-04 - accuracy: 1.0000\n",
      "Epoch 67: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.1101e-04 - accuracy: 1.0000 - val_loss: 0.0923 - val_accuracy: 0.9822\n",
      "Epoch 68/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 1.0911e-04 - accuracy: 1.0000\n",
      "Epoch 68: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.0915e-04 - accuracy: 1.0000 - val_loss: 0.0926 - val_accuracy: 0.9823\n",
      "Epoch 69/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.0708e-04 - accuracy: 1.0000\n",
      "Epoch 69: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.0703e-04 - accuracy: 1.0000 - val_loss: 0.0930 - val_accuracy: 0.9823\n",
      "Epoch 70/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 1.0561e-04 - accuracy: 1.0000\n",
      "Epoch 70: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.0523e-04 - accuracy: 1.0000 - val_loss: 0.0927 - val_accuracy: 0.9823\n",
      "Epoch 71/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 1.0312e-04 - accuracy: 1.0000\n",
      "Epoch 71: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.0345e-04 - accuracy: 1.0000 - val_loss: 0.0931 - val_accuracy: 0.9822\n",
      "Epoch 72/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 1.0120e-04 - accuracy: 1.0000\n",
      "Epoch 72: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.0156e-04 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 0.9822\n",
      "Epoch 73/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 9.9538e-05 - accuracy: 1.0000\n",
      "Epoch 73: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 9.9543e-05 - accuracy: 1.0000 - val_loss: 0.0931 - val_accuracy: 0.9822\n",
      "Epoch 74/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 9.8315e-05 - accuracy: 1.0000\n",
      "Epoch 74: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 9.8207e-05 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9822\n",
      "Epoch 75/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 9.6101e-05 - accuracy: 1.0000\n",
      "Epoch 75: val_accuracy did not improve from 0.98275\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 9.6065e-05 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9823\n",
      "Epoch 76/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 9.4763e-05 - accuracy: 1.0000\n",
      "Epoch 76: val_accuracy did not improve from 0.98275\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 9.4839e-05 - accuracy: 1.0000 - val_loss: 0.0937 - val_accuracy: 0.9822\n",
      "Epoch 76: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = (28,28), name = 'in_flat'),\n",
    "    keras.layers.Dense(300,activation = 'relu'),\n",
    "    keras.layers.Dense(100,activation = 'relu'),\n",
    "    keras.layers.Dense(10,activation = 'softmax'),\n",
    "])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('opt_mnist_model.h5',monitor = 'val_accuracy',verbose = 1,save_best_only = True)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor = 'val_accuracy',verbose = 1, patience = 20,restore_best_weights = True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy',optimizer = keras.optimizers.SGD(learning_rate = opt_lr),metrics = ['accuracy'])\n",
    "history = model.fit(X_train,y_train, epochs = 100, validation_data = (X_val,y_val),\n",
    "         callbacks = [checkpoint_cb,earlystop_cb,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0830 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08299919962882996, 0.9822999835014343]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 0.2395 - accuracy: 0.9254\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94675, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2391 - accuracy: 0.9255 - val_loss: 0.1692 - val_accuracy: 0.9467\n",
      "Epoch 2/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9692\n",
      "Epoch 2: val_accuracy improved from 0.94675 to 0.97142, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0996 - accuracy: 0.9693 - val_loss: 0.0924 - val_accuracy: 0.9714\n",
      "Epoch 3/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9794\n",
      "Epoch 3: val_accuracy improved from 0.97142 to 0.97233, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0660 - accuracy: 0.9794 - val_loss: 0.0927 - val_accuracy: 0.9723\n",
      "Epoch 4/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9832\n",
      "Epoch 4: val_accuracy improved from 0.97233 to 0.97608, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0516 - accuracy: 0.9833 - val_loss: 0.0824 - val_accuracy: 0.9761\n",
      "Epoch 5/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.0378 - accuracy: 0.9877\n",
      "Epoch 5: val_accuracy did not improve from 0.97608\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0378 - accuracy: 0.9876 - val_loss: 0.0962 - val_accuracy: 0.9742\n",
      "Epoch 6/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 0.0292 - accuracy: 0.9907\n",
      "Epoch 6: val_accuracy improved from 0.97608 to 0.97792, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0294 - accuracy: 0.9907 - val_loss: 0.0848 - val_accuracy: 0.9779\n",
      "Epoch 7/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9931\n",
      "Epoch 7: val_accuracy did not improve from 0.97792\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0230 - accuracy: 0.9931 - val_loss: 0.1011 - val_accuracy: 0.9745\n",
      "Epoch 8/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9935\n",
      "Epoch 8: val_accuracy did not improve from 0.97792\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.1050 - val_accuracy: 0.9735\n",
      "Epoch 9/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9956\n",
      "Epoch 9: val_accuracy improved from 0.97792 to 0.97917, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.0979 - val_accuracy: 0.9792\n",
      "Epoch 10/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9970\n",
      "Epoch 10: val_accuracy improved from 0.97917 to 0.98250, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.0851 - val_accuracy: 0.9825\n",
      "Epoch 11/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9975\n",
      "Epoch 11: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.1096 - val_accuracy: 0.9772\n",
      "Epoch 12/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9960\n",
      "Epoch 12: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.0994 - val_accuracy: 0.9791\n",
      "Epoch 13/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9974\n",
      "Epoch 13: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.1019 - val_accuracy: 0.9808\n",
      "Epoch 14/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9966\n",
      "Epoch 14: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.1301 - val_accuracy: 0.9746\n",
      "Epoch 15/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9971\n",
      "Epoch 15: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 0.1021 - val_accuracy: 0.9800\n",
      "Epoch 16/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9973\n",
      "Epoch 16: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 0.1049 - val_accuracy: 0.9791\n",
      "Epoch 17/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9978\n",
      "Epoch 17: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.1125 - val_accuracy: 0.9795\n",
      "Epoch 18/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9985\n",
      "Epoch 18: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.1279 - val_accuracy: 0.9789\n",
      "Epoch 19/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9987\n",
      "Epoch 19: val_accuracy did not improve from 0.98250\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.1157 - val_accuracy: 0.9819\n",
      "Epoch 20/100\n",
      "1480/1500 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 20: val_accuracy improved from 0.98250 to 0.98358, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0988 - val_accuracy: 0.9836\n",
      "Epoch 21/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 6.0060e-04 - accuracy: 0.9999\n",
      "Epoch 21: val_accuracy improved from 0.98358 to 0.98375, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 5.9938e-04 - accuracy: 0.9999 - val_loss: 0.0989 - val_accuracy: 0.9837\n",
      "Epoch 22/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 9.7641e-05 - accuracy: 1.0000\n",
      "Epoch 22: val_accuracy improved from 0.98375 to 0.98392, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 9.7478e-05 - accuracy: 1.0000 - val_loss: 0.1000 - val_accuracy: 0.9839\n",
      "Epoch 23/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 6.9267e-05 - accuracy: 1.0000\n",
      "Epoch 23: val_accuracy did not improve from 0.98392\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 6.9258e-05 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 0.9839\n",
      "Epoch 24/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 5.8085e-05 - accuracy: 1.0000\n",
      "Epoch 24: val_accuracy did not improve from 0.98392\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 5.8338e-05 - accuracy: 1.0000 - val_loss: 0.1019 - val_accuracy: 0.9839\n",
      "Epoch 25/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 5.0768e-05 - accuracy: 1.0000\n",
      "Epoch 25: val_accuracy improved from 0.98392 to 0.98408, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 5.0811e-05 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9841\n",
      "Epoch 26/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 4.5449e-05 - accuracy: 1.0000\n",
      "Epoch 26: val_accuracy improved from 0.98408 to 0.98417, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 4.5464e-05 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 0.9842\n",
      "Epoch 27/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 4.1485e-05 - accuracy: 1.0000\n",
      "Epoch 27: val_accuracy did not improve from 0.98417\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 4.1345e-05 - accuracy: 1.0000 - val_loss: 0.1038 - val_accuracy: 0.9842\n",
      "Epoch 28/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 3.7891e-05 - accuracy: 1.0000\n",
      "Epoch 28: val_accuracy improved from 0.98417 to 0.98425, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 3.7836e-05 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9843\n",
      "Epoch 29/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 3.5240e-05 - accuracy: 1.0000\n",
      "Epoch 29: val_accuracy did not improve from 0.98425\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 3.5155e-05 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9842\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 3.2843e-05 - accuracy: 1.0000\n",
      "Epoch 30: val_accuracy did not improve from 0.98425\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 3.2843e-05 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9841\n",
      "Epoch 31/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 3.0859e-05 - accuracy: 1.0000\n",
      "Epoch 31: val_accuracy did not improve from 0.98425\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 3.0761e-05 - accuracy: 1.0000 - val_loss: 0.1057 - val_accuracy: 0.9842\n",
      "Epoch 32/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 2.9035e-05 - accuracy: 1.0000\n",
      "Epoch 32: val_accuracy did not improve from 0.98425\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.9048e-05 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 0.9843\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 2.7376e-05 - accuracy: 1.0000\n",
      "Epoch 33: val_accuracy improved from 0.98425 to 0.98433, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.7376e-05 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9843\n",
      "Epoch 34/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 2.6167e-05 - accuracy: 1.0000\n",
      "Epoch 34: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.6134e-05 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9843\n",
      "Epoch 35/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 2.5066e-05 - accuracy: 1.0000\n",
      "Epoch 35: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.4883e-05 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9843\n",
      "Epoch 36/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 2.3649e-05 - accuracy: 1.0000\n",
      "Epoch 36: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.3736e-05 - accuracy: 1.0000 - val_loss: 0.1075 - val_accuracy: 0.9843\n",
      "Epoch 37/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 2.2795e-05 - accuracy: 1.0000\n",
      "Epoch 37: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.2780e-05 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 0.9843\n",
      "Epoch 38/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 2.1809e-05 - accuracy: 1.0000\n",
      "Epoch 38: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.1854e-05 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9843\n",
      "Epoch 39/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 2.1000e-05 - accuracy: 1.0000\n",
      "Epoch 39: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.1015e-05 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.9843\n",
      "Epoch 40/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 2.0292e-05 - accuracy: 1.0000\n",
      "Epoch 40: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 2.0277e-05 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.9843\n",
      "Epoch 41/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 1.9620e-05 - accuracy: 1.0000\n",
      "Epoch 41: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.9541e-05 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 0.9843\n",
      "Epoch 42/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 1.8924e-05 - accuracy: 1.0000\n",
      "Epoch 42: val_accuracy did not improve from 0.98433\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.8916e-05 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9843\n",
      "Epoch 43/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 1.8358e-05 - accuracy: 1.0000\n",
      "Epoch 43: val_accuracy improved from 0.98433 to 0.98442, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.8296e-05 - accuracy: 1.0000 - val_loss: 0.1093 - val_accuracy: 0.9844\n",
      "Epoch 44/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 1.7721e-05 - accuracy: 1.0000\n",
      "Epoch 44: val_accuracy did not improve from 0.98442\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.7739e-05 - accuracy: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.9843\n",
      "Epoch 45/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 1.7243e-05 - accuracy: 1.0000\n",
      "Epoch 45: val_accuracy did not improve from 0.98442\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.7197e-05 - accuracy: 1.0000 - val_loss: 0.1098 - val_accuracy: 0.9844\n",
      "Epoch 46/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 1.6699e-05 - accuracy: 1.0000\n",
      "Epoch 46: val_accuracy did not improve from 0.98442\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.6702e-05 - accuracy: 1.0000 - val_loss: 0.1100 - val_accuracy: 0.9844\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.6227e-05 - accuracy: 1.0000\n",
      "Epoch 47: val_accuracy did not improve from 0.98442\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.6227e-05 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9843\n",
      "Epoch 48/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.5798e-05 - accuracy: 1.0000\n",
      "Epoch 48: val_accuracy did not improve from 0.98442\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5797e-05 - accuracy: 1.0000 - val_loss: 0.1104 - val_accuracy: 0.9844\n",
      "Epoch 49/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 1.5367e-05 - accuracy: 1.0000\n",
      "Epoch 49: val_accuracy did not improve from 0.98442\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5380e-05 - accuracy: 1.0000 - val_loss: 0.1106 - val_accuracy: 0.9843\n",
      "Epoch 50/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 1.4998e-05 - accuracy: 1.0000\n",
      "Epoch 50: val_accuracy did not improve from 0.98442\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.4992e-05 - accuracy: 1.0000 - val_loss: 0.1108 - val_accuracy: 0.9844\n",
      "Epoch 51/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 1.4599e-05 - accuracy: 1.0000\n",
      "Epoch 51: val_accuracy improved from 0.98442 to 0.98450, saving model to ageron_opt_mnist_model.h5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.4600e-05 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9845\n",
      "Epoch 52/100\n",
      "1490/1500 [============================>.] - ETA: 0s - loss: 1.4256e-05 - accuracy: 1.0000\n",
      "Epoch 52: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.4268e-05 - accuracy: 1.0000 - val_loss: 0.1112 - val_accuracy: 0.9845\n",
      "Epoch 53/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.3892e-05 - accuracy: 1.0000\n",
      "Epoch 53: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.3934e-05 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9845\n",
      "Epoch 54/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 1.3583e-05 - accuracy: 1.0000\n",
      "Epoch 54: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.3612e-05 - accuracy: 1.0000 - val_loss: 0.1116 - val_accuracy: 0.9845\n",
      "Epoch 55/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 1.3322e-05 - accuracy: 1.0000\n",
      "Epoch 55: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.3321e-05 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9845\n",
      "Epoch 56/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 1.3020e-05 - accuracy: 1.0000\n",
      "Epoch 56: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.3017e-05 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9845\n",
      "Epoch 57/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 1.2774e-05 - accuracy: 1.0000\n",
      "Epoch 57: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.2746e-05 - accuracy: 1.0000 - val_loss: 0.1121 - val_accuracy: 0.9845\n",
      "Epoch 58/100\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 1.2494e-05 - accuracy: 1.0000\n",
      "Epoch 58: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.2478e-05 - accuracy: 1.0000 - val_loss: 0.1122 - val_accuracy: 0.9844\n",
      "Epoch 59/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.2230e-05 - accuracy: 1.0000\n",
      "Epoch 59: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.2228e-05 - accuracy: 1.0000 - val_loss: 0.1123 - val_accuracy: 0.9844\n",
      "Epoch 60/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 1.1967e-05 - accuracy: 1.0000\n",
      "Epoch 60: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.1983e-05 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9844\n",
      "Epoch 61/100\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 1.1809e-05 - accuracy: 1.0000\n",
      "Epoch 61: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.1748e-05 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.9844\n",
      "Epoch 62/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 1.1443e-05 - accuracy: 1.0000\n",
      "Epoch 62: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.1529e-05 - accuracy: 1.0000 - val_loss: 0.1128 - val_accuracy: 0.9844\n",
      "Epoch 63/100\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 1.1318e-05 - accuracy: 1.0000\n",
      "Epoch 63: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.1308e-05 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9844\n",
      "Epoch 64/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 1.1147e-05 - accuracy: 1.0000\n",
      "Epoch 64: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.1105e-05 - accuracy: 1.0000 - val_loss: 0.1131 - val_accuracy: 0.9844\n",
      "Epoch 65/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 1.0914e-05 - accuracy: 1.0000\n",
      "Epoch 65: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.0902e-05 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9844\n",
      "Epoch 66/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 1.0783e-05 - accuracy: 1.0000\n",
      "Epoch 66: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.0720e-05 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 0.9844\n",
      "Epoch 67/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 1.0553e-05 - accuracy: 1.0000\n",
      "Epoch 67: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.0531e-05 - accuracy: 1.0000 - val_loss: 0.1135 - val_accuracy: 0.9844\n",
      "Epoch 68/100\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 1.0362e-05 - accuracy: 1.0000\n",
      "Epoch 68: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.0358e-05 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 0.9844\n",
      "Epoch 69/100\n",
      "1487/1500 [============================>.] - ETA: 0s - loss: 1.0179e-05 - accuracy: 1.0000\n",
      "Epoch 69: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.0183e-05 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9844\n",
      "Epoch 70/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 1.0036e-05 - accuracy: 1.0000\n",
      "Epoch 70: val_accuracy did not improve from 0.98450\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.0020e-05 - accuracy: 1.0000 - val_loss: 0.1139 - val_accuracy: 0.9845\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 9.8610e-06 - accuracy: 1.0000\n",
      "Epoch 71: val_accuracy did not improve from 0.98450\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 9.8610e-06 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9845\n",
      "Epoch 71: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = (28,28), name = 'in_flat'),\n",
    "    keras.layers.Dense(300,activation = 'relu'),\n",
    "    keras.layers.Dense(100,activation = 'relu'),\n",
    "    keras.layers.Dense(10,activation = 'softmax'),\n",
    "])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('ageron_opt_mnist_model.h5',monitor = 'val_accuracy',verbose = 1,save_best_only = True)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor = 'val_accuracy',verbose = 1, patience = 20,restore_best_weights = True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy',optimizer = keras.optimizers.SGD(learning_rate = 3e-1),metrics = ['accuracy'])\n",
    "history = model.fit(X_train,y_train, epochs = 100, validation_data = (X_val,y_val),\n",
    "         callbacks = [checkpoint_cb,earlystop_cb,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1049 - accuracy: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1049141138792038, 0.9848999977111816]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = keras.models.load_model('opt_mnist_model.h5')\n",
    "\n",
    "rnsamp = np.random.choice(range(len(X_test)),4)\n",
    "preds = my_model.predict(X_test[rnsamp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAADECAYAAAC/QeFrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVi0lEQVR4nO3dfbCVZd3o8etW8A1nUGQb+OihF+Wl8NHAKEvS8WQezvFlk5o1gDGlliUwcU4KIpQZEFOC5hPDZFkomiPDhuAZTZweiXDUI2OoiHhGPNkcSmNjKY3gC9znD/zjGdfvrr1w7732XtfnM+M/37m69+UeLtb6ufa+KsqyTAAAADk6qNEbAAAAaBQDEQAAkC0DEQAAkC0DEQAAkC0DEQAAkC0DEQAAkC0DEQAAkC0DUTcpiuL9RVHcVxTFX4uieKkoin8riqJPo/cFzaYoihFFUfxHURSvFkXxfFEU4xu9J2g2RVGsK4piT1EUf3/nn+cavSdoNt47dh8DUfdZnFL6S0ppcErp1JTSmSmlrzdyQ9Bs3nmh+FVK6d9TSgNSSlemlJYVRTG0oRuD5nR1WZZHvvPPsEZvBpqQ947dxEDUfT6QUrq3LMs9ZVm+lFL6dUrpIw3eEzSb4Sml41JKi8qy3FuW5X+klB5OKU1q7LYAoG7eO3YTA1H3uTml9IWiKI4oiuJfUkrj0v4/2EDXKlJKIxu9CWhC84uiaC+K4uGiKM5q9GagCd2cvHfsFgai7rM+7Z/qX0sp/b+U0saU0qpGbgia0HNp/48XfKsoir5FUXw27f8RgyMauy1oOtemlD6YUvqXlNJPUkpriqL4UGO3BE3He8duYiDqBkVRHJT2T/RtKaV+KaWBKaWjU0oLGrkvaDZlWb6VUmpNKf2PlNJLKaX/mVK6N+1/IQE6SVmWj5VluassyzfKslya9v9o6n9v9L6gWXjv2L0MRN1jQErpv6SU/u2dF4+dKaWfJy8e0OnKsnyqLMszy7I8pizLc9P+/4r9vxu9L2hyZdr/46lA5/DesRsZiLpBWZbtKaX/m1K6qiiKPkVRHJVS+lJK6amGbgyaUFEU/1oUxWHv/Mz1/0r7b+f5RYO3BU2jKIqjiqI4951z1qcoigkppU8nv9sAncZ7x+5lIOo+n0sp/beU0o6U0vMppbdSSt9s6I6gOU1KKf057f9dov+aUjqnLMs3GrslaCp9U0rfS/tfz9pTSlNSSq1lWf6fhu4Kmo/3jt2kKMuy0XsAAABoCJ8QAQAA2TIQAQAA2TIQAQAA2TIQAQAA2epTz+KiKNzAQI9RlmVT/n9eOGf0JM4ZdL1mPWcpOWv0LFVnzSdEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtgxEAABAtvo0egMAQB769u0b9hEjRtS0Sy65pK5nn3rqqWE/77zz6nrOI488EvaxY8fWtL1799b1bPhnDj744LBfddVVYb/++uvD/r73vS/sGzdurGnRn+2UUtqzZ0/Ym5FPiAAAgGwZiAAAgGwZiAAAgGwZiAAAgGwZiAAAgGwVZVl2fHFRdHxxL9fS0lLTbrzxxnDt1q1bw37llVeGvep7ftZZZ4V9x44dYe9JlixZEvbx48fXtNmzZ4drf/KTn9T1NcuyLOr6H/QSOZ0zej7njJRSOvzww8P+pS99Kewf+MAHwj5x4sSwDx48+MA21o0OPfTQmvbWW291yrOb9Zyl5KzVa8WKFWFvbW0N+/bt28O+ZcuWsJ922mk1bcyYMeHaF154Iey9WdVZ8wkRAACQLQMRAACQLQMRAACQLQMRAACQLQMRAACQrT6N3kCjRbfJpZTSfffdV9NGjRoVrm1rawt71W1yw4YNC/vMmTPDPn369LB3parvyx133BH2z372s2GPvgdVN58A0D369Ilf/q+44oqwf+tb3wr7+9///s7a0nv2t7/9Lez79u2r6zkLFizolOdASikde+yxYb/hhhvCfsEFF4S96j3lwoULw37rrbd2eD9//vOfw7U58QkRAACQLQMRAACQLQMRAACQLQMRAACQLQMRAACQrexvmZs6dWrYoxvlnnjiiXDtVVddFfb29vawV90IMnbs2LB3hn79+oV9xowZYZ81a1bYq245ef3118N+2WWX1bQNGzaEayGllE455ZSwT5kyJeyf+tSnwj58+PC6vu5bb70V9qVLl9a0m266KVy7devWur4mHIj+/fuHff78+R1+xoABA8L++c9//oD21FE7d+4M+9NPP13Tli9fXtezly1bFvZdu3bV9Rw4UAcdVPs5w9VXXx2uvfLKK8O+Y8eOsN9888119SrRjXInnnhiuPbVV18Ne9UeezOfEAEAANkyEAEAANkyEAEAANkyEAEAANkyEAEAANkqqm4NCxcXRccX9zBVN04988wzYY++L4MGDQrXVt0mV6XqZpFjjjkm7PXcHFRlxYoVYb/wwgvDXhRF2Kv+vFxyySVhX7lyZQd2d2DKsow32cv15nNW5eijjw77t7/97Zo2YcKEcG3V+aj35sMq0e1AKaV0+OGH17SqG3YmTpwY9nXr1oW96ma7nsQ563mqXkOWLFnSZV/zL3/5S9ife+65sN9xxx1hX7NmTV3Pz0WznrOUevdZq9ekSZNq2i9+8Yu6nvHlL3857NGNp//IoYceGvbLL7+8ps2bNy9cW9UXLFhQ1156kqqz5hMiAAAgWwYiAAAgWwYiAAAgWwYiAAAgWwYiAAAgW30avYHuMmLEiLBX3aYW3Y5W721yVdavXx/2rVu31vWcIUOG1LSqm33Gjh0b9qobuubMmRP2uXPndnB35GjgwIFhX7hwYdirbmWL/P73vw/79ddfH/b777+/w89OKaVDDjkk7IsXL65pVTfhPfDAA2G/7rrrwv7973+/g7sjR9Hf8SmlNH369Lqe89JLL9W0Xbt2hWvvvffesFfdYLd9+/a69gLN7Pzzz+/w2ra2trAvW7asrq/Zp0/8Vv7cc88N+49+9KOa9vbbb4drN27cWNdeejOfEAEAANkyEAEAANkyEAEAANkyEAEAANkyEAEAANnK5pa5KlW3rFX1zlDvbXLDhw8P+4oVK2rasGHDwrVVN9vNmzcv7GvXru3g7sjRuHHjwn777beHvaWlJeyPP/54Tav6M1l1g9uePXvCXq8333wz7JdffnlN2717d7j2G9/4RthPOeWUA98Y2frmN78Z9qFDh9b1nGuvvbam3XnnnQe0J6Da6aef3uG1N998c9j37t1b19ecMmVK2H/4wx92+BmTJ08O+29+85u69tKb+YQIAADIloEIAADIloEIAADIloEIAADIVjaXKlT9UndRFHX1rjR69Oiw33fffWGP/p3mzp0brp09e/aBb4xsVf1S93e/+92w9+vXL+y//OUvwz5hwoQD21iDXXDBBXWtP++888I+cuTIsG/evLnuPQHQe/z4xz8O+1lnnRX2M888M+zf+c536vq60YVca9asqesZzcgnRAAAQLYMRAAAQLYMRAAAQLYMRAAAQLYMRAAAQLayuWVuy5YtYS/LMuytra01beHCheHaefPmhb29vT3s48ePD/uSJUvCfswxx4S9ra2tps2fPz9cC//IuHHjwl51a+Frr70W9quvvjrsy5YtO7CNNYmq2/cGDhzYzTshRyeeeGJN+8QnPlHXM774xS92+Nn/yO233x727du3d/gZmzZtCvuePXvq2gt0tq985Ss17YYbbgjXjhkzJuxPP/102KveUx555JFhX7VqVdgnTZpU0954441wbU58QgQAAGTLQAQAAGTLQAQAAGTLQAQAAGTLQAQAAGSrqLplLVxcFB1f3Et8+tOfDvvMmTNr2rnnnhuurfoebtiwIezDhw8Pe0tLS9ij2+RSSuniiy8Oey7KsiwavYeu0Ihz9tRTT4V95MiRYf/qV78a9ttuu63T9tSTvfjii2E/4YQT6nrO2WefHfZ169bVu6Uu45x1vSOOOCLszz77bNjr/XPWbB555JGwT5s2LewbN27syu10imY9Zyn1rLPWCAMGDAj7ypUrw37GGWfU9fyHH3447J/5zGfC/uabb9b1/GZTddZ8QgQAAGTLQAQAAGTLQAQAAGTLQAQAAGTLQAQAAGSrT6M30Gjr16/vcK+6ZS66kS6llMaOHRv2qlvpduzYEfbp06eHHeo1ZMiQsA8ePDjs27ZtC/vy5cs7bU89xeGHHx72n//85zXt+OOPr+vZTzzxRNirvr+QUvVrRVfatWtX2B977LGwjxo1KuwvvPBC2E866aSw9+/fvwO72+/0008P++rVq8P+gx/8IOyLFi3q8NeEA/XKK6+EffHixWGv95a5omjaCwq7lU+IAACAbBmIAACAbBmIAACAbBmIAACAbBmIAACAbBX13GJTFEX3X3nTg/Tr1y/sd9xxR9jHjx8f9qrv+dq1a8M+adKksLe3t4c9F2VZNuXVKl15zj70oQ+F/fHHH6/aS9ivueaasN92220HtrEucP7554f9wx/+cNhbW1vD/vGPf7ymVX1fqs72scceG/becIads8a54oorwj5r1qywv/rqq2H/4x//WNPa2trCtVV/F2zevDnsw4cPD/uf/vSnsJ9wwglhP+2002ra1KlTw7Uf/ehHw17l5ZdfDvuwYcPC/tprr9X1/M7QrOcspd5x1rrScccdF/Z169aFvep1+vXXXw/7EUccEfaVK1eG/eKLLw57LqrOmk+IAACAbBmIAACAbBmIAACAbBmIAACAbBmIAACAbLllrg5DhgwJ+2OPPRb2lpaWsFd9z3fv3h32yy67LOxVN4jkollv5WnEOVu+fHnYL7roorqe8/bbb4f97rvvDnvVrTn1OOOMM8I+cuTIup6zb9++sG/atKmmnXzyyeHavn37ht0tcz1Pb349q7qpreo87dy5syu302W+8IUvhL3q75N6DRw4MOyvvPJKpzy/Hs16zlLq3WetM8ycOTPs3/ve98L+u9/9LuwLFiwI+z333BP2qtejs88+u6Y9+uij4dpm5JY5AACAdzEQAQAA2TIQAQAA2TIQAQAA2TIQAQAA2XLLXB2WLFkS9iuuuCLsa9euDfuOHTvCPnHixLA/++yzYf/IRz4S9lw06608jThnBx0U/7eRadOmhX327NlhP+qoozprSx1WdbPdli1bwt7W1hb2X/3qV2F/8skna9qLL74Yrq26/cstcz1PI87ZwQcfHPbW1tawr1ixogt307Mcf/zxNW316tXh2lNPPbWuZz///PNhHzVqVNj//ve/1/X8ztCs5yylvN47jh49uqb9+te/DtcOGDAg7IMGDQp71XvHqhviPvaxj4V91apVNa3eG2V7M7fMAQAAvIuBCAAAyJaBCAAAyJaBCAAAyFafRm+g0VpaWsI+c+bMmjZ+/PhwbdXFFOvXrw/7zp07wz5hwoSw79u3L+zQWar+jC1atCjsDz74YNirfkm5Ky1btizszg09zdSpU8M+d+7csD/zzDNh37p1a6ftqbuNGzcu7NH3oN7LE6rcdNNNYW/E5Qk0t3POOaemVV2esHTp0rB39WU7Ve97c+cTIgAAIFsGIgAAIFsGIgAAIFsGIgAAIFsGIgAAIFvZ3zI3ZMiQsE+bNq2mFUURrq26IWj+/Plhv/POO8Ne9fwNGzaEHRpl8+bNdfXcnXzyyWF/6KGHunknNFLVbVOHHXZY2GfMmBH2yZMnd9aW3rPBgweHfcqUKWGfPn162A855JD3vJd77rkn7D/96U/f87OhI6puUYz87Gc/C3vVzcVDhw4N+0knndThr5lSSvfff39d63PhEyIAACBbBiIAACBbBiIAACBbBiIAACBbBiIAACBb2d8yd+GFF4Y9uuWjvb09XHvbbbeFffjw4WFvbW3t8NdMKaWVK1eGHWiM7du3h/2EE04I++c+97mwu2UuL4sWLQr7rFmzwj5p0qSwf/KTn6zr+fW49NJLwz5s2LCw9+3bN+xVN+p1hrvuuivsVd/HvXv3dtle4D/74Ac/2GXP/vrXvx72o446KuyvvPJK2BcvXtxZW2oqPiECAACyZSACAACyZSACAACyZSACAACyZSACAACyVVTdbBYuLoqOL+4lqv799+3bV9NuueWWcO38+fPD/tvf/jbsVbf1rF27Nuzjxo0Le+7KsiwavYeu0IznrNmcd955YV+9enXY//rXv4Z99OjRYf/DH/5wQPvqCs5Z5zn00EPDvnv37m7eSc+zbdu2mrZmzZpwbdVr7o4dOzp1T92pWc9ZSnm9pj3zzDM1rerG4blz54b97rvvDvuKFSvCXvX86EyllNLQoUPDnouqs+YTIgAAIFsGIgAAIFsGIgAAIFsGIgAAIFsGIgAAIFvZ3zIX3QiSUnwTXNXNHyNGjAj7qFGjwl71PR80aFDY29vbw567Zr2VpxnPWbM57LDDwn7nnXeG/aKLLgr7XXfdFfbJkyeHfe/evf98c53MOevUrxn2o48+OuwPPPBA2KtuJ+wNli1bFvY5c+bUtJ5022JXa9ZzllJer2kzZsyoaVW3yXWWhx56KOxf+9rXwv7888935XZ6PLfMAQAAvIuBCAAAyJaBCAAAyJaBCAAAyJaBCAAAyFb2t8ytWLEi7K2trTXtoIPi+XHfvn1hv+WWW8I+b968sLtNrj7NeitPM56zXJxzzjlhv//++8N+8MEHh73qxsmXX375wDb2HjhnjXPkkUeG/Zprrgl7//79wz5lypSa1tbWFq7dsmVL2KtuWd22bVvYq1TdlFj1OpqLZj1nKfWOs9ZZBgwYUNMuvfTScO11110X9uOOOy7sDz74YNivvfbasD/55JNhz51b5gAAAN7FQAQAAGTLQAQAAGTLQAQAAGTLQAQAAGQr+1vmhgwZEvalS5fWtJaWlnBt1U11c+bMOfCN8U816608zXjOcvfoo4+GfcyYMWGfNm1a2G+99dZO21NHOWfQ9Zr1nKXkrNGzuGUOAADgXQxEAABAtgxEAABAtgxEAABAtrK/VIHeq1l/CdU5az4XXHBB2FetWhX2TZs2hX3UqFGdtKOOc86g6zXrOUvJWaNncakCAADAuxiIAACAbBmIAACAbBmIAACAbBmIAACAbPVp9AYAmt3atWvDfuONN4Z96NChXbkdAOA/8QkRAACQLQMRAACQLQMRAACQLQMRAACQLQMRAACQraIsy44vLoqOL4YuVpZl0eg9dAXnjJ7EOYOu16znLCVnjZ6l6qz5hAgAAMiWgQgAAMiWgQgAAMiWgQgAAMiWgQgAAMhWnzrXt6eUXuyKjUCdhjR6A13IOaOncM6g6zXzOUvJWaPnqDxrdV27DQAA0Ez8yBwAAJAtAxEAAJAtAxEAAJAtAxEAAJAtAxEAAJAtAxEAAJAtAxEAAJAtAxEAAJAtAxEAAJCt/w9fi47bVtvF9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1080x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncols = preds.shape[0]\n",
    "nrows = 1\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for s in range(len(rnsamp)):\n",
    "    plt.subplot(nrows,ncols,s+1); \n",
    "    plt.imshow(X_test[rnsamp[s]],cmap = 'gray'); plt.xticks([]); plt.yticks([]); plt.title(np.argmax(preds[s]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "826d579131482cc18925d10bb205e43407f6afd7b98a12f47d1a792c1d5ef477"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
